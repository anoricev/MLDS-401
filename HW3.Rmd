---
title: "MLDS 401: Homework 3"
author: "Arielle Weinstein, Jack Bailey, Junbo (Jacob) Lian, Veronica Lin"
date: "2025-10-01"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: true
fontsize: 11pt
geometry: margin=1in
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.height = 4.4,
  fig.width  = 6.8,
  fig.align  = "center",
  message = FALSE,
  warning = FALSE
)
set.seed(2025)
```

```{r packages}
# Install once in Console if missing:
# install.packages(c("ggplot2","dplyr","broom","sandwich","lmtest","knitr"))
need <- c("ggplot2","dplyr","broom","sandwich","lmtest","knitr")
miss <- setdiff(need, rownames(installed.packages()))
if(length(miss)) install.packages(miss, quiet = TRUE)
invisible(lapply(need, library, character.only = TRUE))

# Use treatment contrasts (default), but fix base level for origin to "US"
options(contrasts = c("contr.treatment", "contr.poly"))
```

```{r data}
data_dir  <- "E:/mlds/MLDS 401 Machine Learning/course_files_export/Homework/HW3"
auto_path <- file.path(data_dir, "auto.txt")
part_path <- file.path(data_dir, "part.csv")

# Fallback to working directory if absolute paths not found
if(!file.exists(auto_path)) auto_path <- "auto.txt"
if(!file.exists(part_path)) part_path <- "part.csv"

# Read data
auto <- read.table(auto_path, header = TRUE, na.strings = c("NA","?",""))
names(auto) <- trimws(names(auto))
auto$origin <- factor(auto$origin, levels = 1:3, labels = c("US","Europe","Japan"))
auto <- auto %>% mutate(lmpg = log(mpg), lw = log(weight))

part <- read.csv(part_path)

# Basic checks
stopifnot(all(c("mpg","weight","year","origin") %in% names(auto)))
stopifnot(all(c("y","x","tx","wc") %in% names(part)))

# Q6 engineered variables
d <- part %>%
  mutate(
    tx  = as.integer(tx),      # ensure 0/1
    ly  = log(y + 1),
    lx  = log(x + 1),
    lwc = log(wc + 1)
  )
```

# 1. (JWHT 3.9) `auto` Data

```{r q1a}
# 1(a)
tab_origin  <- table(auto$origin, useNA = "ifany")
prop_origin <- prop.table(tab_origin)
knitr::kable(as.data.frame.matrix(rbind(Freq = tab_origin, Prop = round(prop_origin,3))),
             caption = "Origin: frequency and proportion")
```

```{r q1b}
# 1(b)
m1 <- lm(mpg ~ origin + weight + year, data = auto)
tidy_m1   <- broom::tidy(m1, conf.int = TRUE)
glance_m1 <- broom::glance(m1)
knitr::kable(tidy_m1, digits = 4, caption = "Model 1: OLS for mpg")
knitr::kable(glance_m1, digits = 4, caption = "Model 1: fit statistics")

# Standard diagnostics
par(mfrow = c(2,2)); plot(m1); par(mfrow = c(1,1))
```

```{r q1c}
# 1(c)
m2 <- lm(lmpg ~ origin + lw + year + I(year^2), data = auto)
tidy_m2   <- broom::tidy(m2, conf.int = TRUE)
glance_m2 <- broom::glance(m2)
knitr::kable(tidy_m2, digits = 4, caption = "Model 2: OLS for log(mpg)")
knitr::kable(glance_m2, digits = 4, caption = "Model 2: fit statistics")

par(mfrow = c(2,2)); plot(m2); par(mfrow = c(1,1))
```

```{r q1d}
# 1(d)
co <- coef(m2); b1 <- co[["year"]]; b2 <- co[["I(year^2)"]]
year_star <- -b1/(2*b2)

grid <- data.frame(year = seq(min(auto$year, na.rm=TRUE),
                              max(auto$year, na.rm=TRUE), by = 0.1))
grid$effect <- b1*grid$year + b2*grid$year^2

ggplot(grid, aes(year, effect)) +
  geom_line() +
  geom_vline(xintercept = year_star, linetype = 2) +
  labs(title = "Quadratic effect of model year on log(mpg)",
       subtitle = paste0("Extremum at year* ≈ ", round(year_star, 2)),
       x = "Model year", y = "β1·year + β2·year^2")

year_star
```

```{r q1e, echo=FALSE}
cat("The coefficient is negative, which indicates that as the weight of the car increases the milage decreases.")
```

# 2. (ACT 3.2) Hat Matrix for SLR

```{r q2, message=FALSE, warning=FALSE}
# Simple linear regression with intercept
x <- auto$year
X <- cbind(1, x)
H <- X %*% solve(t(X) %*% X) %*% t(X)

# Closed form: H_ij = 1/n + (x_i - xbar)(x_j - xbar) / Sxx
n <- length(x); xbar <- mean(x); Sxx <- sum((x - xbar)^2)
H2 <- outer(x, x, function(xi, xj) 1/n + ((xi - xbar)*(xj - xbar))/Sxx)

# Numerical checks expected for a hat matrix
checks <- c(
  `max|H - H2|` = max(abs(H - H2)),
  symmetry      = max(abs(H - t(H))),
  idempotent    = max(abs(H %*% H - H)),
  `trace(H)=p=2`= abs(sum(diag(H)) - 2)
)
knitr::kable(as.data.frame(t(checks)), digits = 6,
             caption = "Hat-matrix properties (should be ≈ 0)")

# Leverage summary
lev <- diag(H)
lev_sum <- summary(lev)
knitr::kable(as.data.frame(t(lev_sum)), digits = 6,
             caption = "Leverage summary (diagonal of H)")
```

# 3. (ACT 3.6) GLS Mean and Covariance (Illustration)

```{r q3}
# GLS estimator
Xg  <- cbind(1, auto$weight)
yg  <- auto$mpg
wvar  <- (auto$weight / mean(auto$weight, na.rm=TRUE))^2  # heteroskedastic pattern
Sigma <- diag(wvar)

beta_gls <- solve(t(Xg) %*% solve(Sigma) %*% Xg) %*% t(Xg) %*% solve(Sigma) %*% yg
colnames(beta_gls) <- NULL; rownames(beta_gls) <- c("(Intercept)","weight")
knitr::kable(beta_gls, col.names = "GLS coef.", digits = 6)

# Sanity
wls <- lm(yg ~ Xg[,2], weights = 1/wvar)
knitr::kable(broom::tidy(wls)[,c("term","estimate")], digits = 6,
             caption = "WLS matches GLS under diagonal Σ")
```

# 4. WLS Intuition with n = 2

```{r q4}
sig1 <- 4; sig2 <- 1
w1 <- (1/sig1^2) / (1/sig1^2 + 1/sig2^2); w2 <- 1 - w1
knitr::kable(t(c(w1 = w1, w2 = w2)), digits = 6,
             caption = "Optimal unbiased weights")
```

# 5. Quadratic Regression: Uncentered vs Centered Equivalence

```{r q5}
# Uncentered
fit_unc  <- lm(mpg ~ year + I(year^2), data = auto)
coef_unc <- coef(fit_unc)

# Correlation before centering
cor_raw <- cor(auto$year, auto$year^2)

# Centering
xbar <- mean(auto$year, na.rm = TRUE)
auto$yc <- auto$year - xbar
cor_cen <- cor(auto$yc, auto$yc^2)

# Visual
g1 <- ggplot(auto, aes(year, I(year^2))) + geom_point(alpha=.6) + theme_minimal() +
  labs(title="Raw: year^2 vs year", x="year", y="year^2")
g2 <- ggplot(auto, aes(yc, I(yc^2))) + geom_point(alpha=.6) + theme_minimal() +
  labs(title="Centered: (year-mean)^2 vs (year-mean)", x="year - mean(year)", y="(year - mean)^2")
g1; g2

# Centered fit and mapping back
fit_cen  <- lm(mpg ~ yc + I(yc^2), data = auto)
coef_cen <- coef(fit_cen)

g0 <- coef_cen["(Intercept)"]; g1c <- coef_cen["yc"]; g2c <- coef_cen["I(yc^2)"]
beta0_hat <- g0 - g1c*xbar + g2c*xbar^2
beta1_hat <- g1c - 2*g2c*xbar
beta2_hat <- g2c

eq_table <- rbind(
  uncentered            = coef_unc,
  recovered_from_center = c("(Intercept)"=beta0_hat, "year"=beta1_hat, "I(year^2)"=beta2_hat)
)
knitr::kable(eq_table, digits = 6, caption = "Uncentered vs recovered-from-centered coefficients")

knitr::kable(t(c(cor_before = cor_raw, cor_after = cor_cen, mean_year = xbar)),
             digits = 6, caption = "Centering reduces correlation between x and x^2")
```

# 6. Social-Media Contest and Post-Period Spending (`part.csv`)

```{r q6}
# Model 1
m6_1 <- lm(ly ~ lx + tx, data = d)
coeftest_m6_1 <- lmtest::coeftest(m6_1, vcov = sandwich::vcovHC(m6_1, type = "HC1"))
knitr::kable(broom::tidy(m6_1, conf.int = TRUE), digits = 4, caption = "Model 1: OLS")
knitr::kable(broom::tidy(coeftest_m6_1), digits = 4, caption = "Model 1: Robust (HC1) t-tests")

# Model 2
m6_2 <- lm(ly ~ lx + tx + lwc, data = d)
coeftest_m6_2 <- lmtest::coeftest(m6_2, vcov = sandwich::vcovHC(m6_2, type = "HC1"))
knitr::kable(broom::tidy(m6_2, conf.int = TRUE), digits = 4, caption = "Model 2: OLS")
knitr::kable(broom::tidy(coeftest_m6_2), digits = 4, caption = "Model 2: Robust t-tests")

# 6(c)
tx_row <- broom::tidy(coeftest_m6_1) %>% dplyr::filter(term == "tx")
knitr::kable(tx_row, digits = 4, caption = "tx effect (Model 1, robust)")

# 6(d)
b_tx <- coef(m6_1)["tx"]
times <- unname(exp(b_tx))
knitr::kable(t(data.frame(`multiplier on (y+1)` = times)), digits = 4)

# (e)-(h)
cat("Adding log(wc+1) markedly attenuates the raw `tx` effect, consistent with omitted-variable bias in Model 1. In Model 2, `lwc` is positive and significant with robust SE, implying that conditional on prior spend `x`, greater word count predicts higher post-period spend. Managerially, contests should incentivize higher-quality, and segment by prior spend for ROI; validate via randomized A/B tests.")
```

# Appendix: Session Info

```{r session}
sessionInfo()
```
